# CentOs7 æ­å»º K8s + é˜¿é‡ŒcGPUåˆ†ç‰‡

æœ¬æ–‡ä»‹ç»å¦‚ä½•åœ¨OCIçš„VMä¸­æ­å»ºK8så¹¶å®‰è£…é˜¿é‡Œäº‘çš„GPUåˆ†ç‰‡å·¥å…·ã€‚ 

## 1. é…ç½®

Master Node: 4 OCPU(8vCPU) 16G 100Gç¡¬ç›˜

GPU Nodeï¼šBM.GPU.A10.4 ï¼Œç£ç›˜100G ï¼ˆè¿™æ˜¯è£¸é‡‘å±ä¸æ˜¯è™šæ‹Ÿæœºï¼Œæ–‡ç« ç¼–å†™æ—¶æ²¡æœ‰åˆé€‚çš„GPUè™šæ‹Ÿæœºå‹å·ï¼Œç°åœ¨å»ºè®®ä½¿ç”¨GPU VMæœºå‹ï¼‰

## 2. æ­å»ºK8s

### Step 1. æ‰©å®¹ç£ç›˜åˆ°100G

åœ¨VMä¸Šæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œè®©Linuxè¯†åˆ«å¤šå‡ºçš„ç£ç›˜ç©ºé—´ï¼š

```bash
sudo yum -y install cloud-utils-growpart gdisk
sudo growpart /dev/sda 3
sudo xfs_growfs /dev/sda3
df -h
```

<aside>
ğŸ’¡ åˆ«ç”¨FinnalShellæ‰§è¡Œï¼Œå¦åˆ™ä¼šå‡ºç°ä¸­æ–‡å¤±è´¥é—®é¢˜

</aside>

### Step 2. å…³é—­å„ç§é™åˆ¶

ä¸‹é¢çš„è¯­å¥ç”¨äºæµ‹è¯•ç¯å¢ƒï¼Œç”Ÿäº§ç¯å¢ƒåº”è°¨æ…ã€‚

```bash
sudo swapoff -a
sudo sed -i '/swap/s/^\(.*\)$/#\1/g' /etc/fstab
sudo setenforce 0

sudo sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
sudo sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
sudo systemctl disable firewalld
sudo systemctl stop firewalld
```

### Step3. å¼€å¯è½¬å‘

```bash
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# sysctl params required by setup, params persist across reboots
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system

lsmod | grep br_netfilter
lsmod | grep overlay

sudo sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward
```

### Step 4. å®‰è£…å®¹å™¨

```bash
sudo yum install -y yum-utils
sudo yum-config-manager  --add-repo  https://download.docker.com/linux/centos/docker-ce.repo
#yum list docker-ce --showduplicates | sort -r

yum clean all
yum makecache

yum install -y yum-utils   device-mapper-persistent-data   lvm2

yum list docker-ce --showduplicates | sort -r

 yum install docker-ce-20.10.0-3.el7 docker-ce-cli-20.10.0-3.el7 containerd.io

sudo yum install docker-ce docker-ce-cli containerd.io docker-compose-plugin -y
sudo systemctl start docker
sudo systemctl enable docker
#sudo docker run hello-world
```

*MasterèŠ‚ç‚¹ä½¿ç”¨containerd+docker-ceçš„é…ç½®è¿˜æœ‰ç‚¹é—®é¢˜ï¼Œè¿™é‡ŒMasteræ¢äº†ä¸€ä¸ªcri-docker è¿è¡Œæ—¶ã€‚ GPUèŠ‚ç‚¹ä¸éœ€è¦æ‰§è¡Œä¸‹é¢çš„æ“ä½œ*

```bash
wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.0/cri-dockerd-0.3.0-3.el7.x86_64.rpm
sudo rpm -ivh cri-dockerd-0.3.0-3.el7.x86_64.rpm

sudo systemctl start cri-docker
sudo systemctl enable cri-docker

ll /var/run/cri-dockerd.sock

#å…³é—­Containerdè¿è¡Œæ—¶
sudo systemctl disable containerd
sudo systemctl stop containerd
```

### Step 5. å®‰è£…K8så·¥å…·

æŒ‡å®šç‰ˆæœ¬ä¸º1.24.3

```bash
cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF

sudo yum install -y kubelet-1.24.3 kubeadm-1.24.3 kubectl-1.24.3 --disableexcludes=kubernetes

sudo systemctl enable --now kubelet
```

### Step 6. åˆ›å»ºK8sé›†ç¾¤

```bash
cat <<EOF | sudo tee kubeadm-config.yaml
kind: ClusterConfiguration
apiVersion: kubeadm.k8s.io/v1beta3
kubernetesVersion: v1.24.3
---
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
cgroupDriver: systemd
EOF

sudo kubeadm init --config kubeadm-config.yaml
```

è®°ä¸‹ä¸‹é¢çš„è¾“å‡ºï¼Œä»¥åä¼šç”¨åˆ°

![tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119174514316.png](tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119174514316.png)

ç»§ç»­

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
kubectl get node -o wide
```

![tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119174715881.png](tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119174715881.png)

### Step 7. GPU Node

å†åˆ›å»ºä¸€å° BM.GPU.A10.4 å‹å·çš„GPU Nodeï¼Œæ‰§è¡ŒStep1 ~ 5.

### Step 8. å®‰è£…é©±åŠ¨

```bash
sudo yum update -y
sudo yum install -y gcc kernel-devel
wget https://us.download.nvidia.com/tesla/515.65.01/NVIDIA-Linux-x86_64-515.65.01.run
chmod +x ./NVIDIA-Linux-x86_64-515.65.01.run
#ç¦ç”¨nouveauï¼Œ
sudo vim /etc/default/grub
# è®¾ç½®å†…æ ¸å‚æ•°modprobe.blacklist=nouveau
sudo grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg
sudo grub2-mkconfig -o /boot/grub2/grub.cfg
sudo reboot

sudo ./NVIDIA-Linux-x86_64-515.65.01.run
#ä¸€ç›´æŒ‰å›è½¦é”®ç¡®å®šå³å¯
nvidia-smi
```

![tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119214526733.png](tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119214526733.png)

### Step 9 . å®‰è£… Nvidia Container Toolkit

```bash
distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
   && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.repo | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo

yum-config-manager --enable libnvidia-container-experimental
sudo yum clean expire-cache
sudo yum install -y nvidia-docker2
sudo systemctl restart docker
#sudo docker run --rm --gpus all nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi
```

ä¿®æ”¹å®¹å™¨è¿è¡Œæ—¶

```bash
cat <<EOF | sudo tee /etc/containerd/config.toml
version = 2
[plugins]
  [plugins."io.containerd.grpc.v1.cri"]
    [plugins."io.containerd.grpc.v1.cri".registry]
       config_path = "/etc/containerd/certs.d"
    [plugins."io.containerd.grpc.v1.cri".containerd]
      default_runtime_name = "nvidia"
      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia]
          privileged_without_host_devices = false
          runtime_engine = ""
          runtime_root = ""
          runtime_type = "io.containerd.runc.v2"
          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia.options]
            BinaryName = "/usr/bin/nvidia-container-runtime"
EOF

# runtime_type = "io.containerd.runtime.v1.linux"

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo systemctl enable kubelet

```

### Step 10. æŠŠGPU NodeåŠ å…¥é›†ç¾¤

åœ¨GPU Nodeä¸Šæ‰§è¡ŒåŠ å…¥æ“ä½œ

```bash
sudo kubeadm join 10.0.10.116:6443 --token sg9835.ouqey7wquuc6kugb \
        --discovery-token-ca-cert-hash sha256:0ae5c99780ba7c41861b4d032a4c462e873f4500cf9ef1dfdcb64b202548570e
```

![tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119184702364.png](tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119184702364.png)

åœ¨Masterä¸Šæ‰§è¡ŒæŸ¥çœ‹

```bash
kubectl get node -o wide
```

![tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119214957132.png](tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119214957132.png)

### Step 11. å®‰è£…å®¹å™¨ç½‘ç»œæ’ä»¶

å‰é¢å¿˜è®°åˆ†é…Podç½‘ç»œäº†ï¼Œç°åŠ ä¸Š

```bash
sudo vim /etc/kubernetes/manifests/kube-controller-manager.yaml
```

```
--allocate-node-cidrs=true--cluster-cidr=10.244.0.0/16
```

![tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119185919636.png](tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119185919636.png)

å®¹å™¨ç½‘ç»œæ’ä»¶CNIç”¨äºè·¨Nodeçš„Podé—´é€šä¿¡ã€‚åœ¨Masterä¸Šæ‰§è¡Œ

```bash
sudo systemctl restart kubelet

curl -O https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl apply -f  kube-flannel.yml

#ç­‰ä¸€ä¼šå„¿
kubectl get pod -A -o wide
```

![tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119190029947.png](tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119190029947.png)

## 3. å®‰è£…é˜¿é‡ŒGPUåˆ†ç‰‡å·¥å…·

### Step 1. å®‰è£…scheduler-extender

```bash
cd /etc/kubernetes/
sudo curl -O https://raw.githubusercontent.com/AliyunContainerService/gpushare-scheduler-extender/master/config/scheduler-policy-config.yaml

kubectl create -f https://raw.githubusercontent.com/AliyunContainerService/gpushare-scheduler-extender/master/config/gpushare-schd-extender.yaml
cd /etc/kubernetes/

#å¤‡ä»½
sudo cp manifests/kube-scheduler.yaml ./kube-scheduler.yaml.bak

cat <<EOF | sudo tee manifests/kube-scheduler.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-scheduler
    tier: control-plane
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
    - --bind-address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --leader-elect=true
    - --config=/etc/kubernetes/scheduler-policy-config.yaml
    image: k8s.gcr.io/kube-scheduler:v1.24.3
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-scheduler
    resources:
      requests:
        cpu: 100m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/kubernetes/scheduler.conf
      name: kubeconfig
      readOnly: true
    - mountPath: /etc/kubernetes/scheduler-policy-config.yaml
      name: scheduler-policy-config
      readOnly: true
  hostNetwork: true
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/scheduler.conf
      type: FileOrCreate
    name: kubeconfig
  - hostPath:
      path: /etc/kubernetes/scheduler-policy-config.yaml
      type: FileOrCreate
    name: scheduler-policy-config
status: {}
EOF
```

æ–‡ä»¶å·®å¼‚å¦‚ä¸‹ï¼š

![tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119191234245.png](tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119191234245.png)

scheduleré…ç½®ä¿®æ”¹åä¼šè‡ªåŠ¨é‡å¯

### Step 2. éƒ¨ç½²Device Plugin

```bash
kubectl create -f https://raw.githubusercontent.com/AliyunContainerService/gpushare-device-plugin/master/device-plugin-rbac.yaml
kubectl create -f https://raw.githubusercontent.com/AliyunContainerService/gpushare-device-plugin/master/device-plugin-ds.yaml
```

ç»™GPUèŠ‚ç‚¹æ‰“æ ‡ç­¾

```bash
kubectl get node
kubectl label node gpu-node gpushare=true
kubectl label node master-28170 node-role.kubernetes.io/master=""
kubectl get node --show-labels
```

![tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119215132607.png](tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119215132607.png)

ç°åœ¨çœ‹çœ‹GPU Nodeä¸Šæœ‰æ²¡æœ‰GPU Shareèµ„æº

```bash
kubectl describe node gpu-node
```

![tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119224504399.png](tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119224504399.png)

*å¦‚æœåœ¨å›½å¤–æ— æ³•æ‹‰å–é˜¿é‡Œäº‘é•œåƒï¼Œé‚£å°±è‡ªè¡Œæ¢ä¸ªåœ°å€ã€‚
ociræ˜¯æˆ‘è‡ªå·±ä¸Šä¼ åˆ°Oracleé•œåƒä»“åº“ï¼Œé‡Œé¢æˆ‘ä¸Šä¼ äº†ä¸€ä¸ªk8s-gpushare-plugin:v2-1.11-aff8a23*

```bash
wget https://raw.githubusercontent.com/AliyunContainerService/gpushare-device-plugin/master/device-plugin-ds.yaml
sudo sed -i 's/registry.cn-hangzhou.aliyuncs.com\/acs/nrt.ocir.io\/sehubjapacprod/g' device-plugin-ds.yaml
kubectl apply -f device-plugin-ds.yaml
```

### Step 3. å®‰è£…æ‰©å±•å·¥å…·

```bash
cd /usr/bin/
sudo wget https://github.com/AliyunContainerService/gpushare-device-plugin/releases/download/v0.3.0/kubectl-inspect-gpushare
sudo chmod u+x /usr/bin/kubectl-inspect-gpushare

#è¿™å·¥å…·æœ‰æƒé™é™åˆ¶ï¼Œæˆ‘ä»¬ç”¨rootç”¨æˆ·è¿è¡Œ
sudo su
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
kubectl inspect gpushare

su opc
```

## 4. æµ‹è¯•

```bash
vim gpu-share-test.yaml
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: binpack-1
  labels:
    app: binpack-1
spec:
  replicas: 2
  selector: # define how the deployment finds the pods it mangages
    matchLabels:
      app: binpack-1

  template: # define the pods specifications
    metadata:
      labels:
        app: binpack-1

    spec:
      containers:
      - name: binpack-1
        image: cheyang/gpu-player:v2
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            # GiB
            aliyun.com/gpu-mem: 2
```

```bash
kubectl apply -f gpu-share-test.yaml
sudo kubectl inspect gpushare
```

![tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119231511735.png](tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119231511735.png)

![tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119231446428.png](tech-doc-master%25E9%25A2%2586%25E5%259F%259FContainerK8s%25E5%259C%25A8K8s%25E4%25B8%25AD%25E4%25BD%25BF%25E7%2594%25A8GPUCentOs7_%25E6%2590%25AD%25E5%25BB%25BA_K8s__GPU%25E5%2588%2586%25E7%2589%2587.assetsimage-20230119231446428.png)